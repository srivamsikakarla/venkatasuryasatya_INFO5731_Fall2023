{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srivamsikakarla/venkatasuryasatya_INFO5731_Fall2023/blob/main/srivamsiIn_class_exercise_02_09_13_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAzh1U0sE5I5"
      },
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpgvZQdRE-HV"
      },
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmNR3kw9-pa"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Research Question:\n",
        "\n",
        "What are the current trending topics or themes among technology enthusiasts based on recent technology-related articles?\n",
        "\n",
        "Type of Data Needed:\n",
        "\n",
        "Titles and content of technology-related articles from a popular technology news website.\n",
        "\n",
        "How Many Data Needed for Analysis:\n",
        "\n",
        "To get a decent representation of current trends, we'd need to analyze a good number of articles. Let's target at least 1000 recent technology news articles.\n",
        "\n",
        "Detailed Steps for Collecting the Data:\n",
        "\n",
        "1. Choose a Popular Technology News Source: A site like TechCrunch is a good candidate since it offers a wide range of articles on emerging technology trends.\n",
        "\n",
        "2. Web Scraping: Using tools like BeautifulSoup and requests, we can programmatically extract titles and content from the articles listed on the site.\n",
        "   Given pagination or the way articles are structured, we might need to scrape multiple pages to collect our target of 1000 articles.\n",
        "\n",
        "\n",
        "3. Data Parsing: After scraping, parse the data to extract relevant information. In our case, the title and possibly a summary or the first few lines of each article would be sufficient.\n",
        "\n",
        "\n",
        "4. Saving Data: we can store the parsed titles (and summaries if chosen) sequentially until we've shown 1000 entries in the form of a list and at the end we can store them\n",
        "in the form of a text file or csv file for further analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "outputId": "1d38c892-48ee-4e67-b92a-4c80281c939a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI startup speeds up the creation of climate-resilient crops\n",
            "SVB’s commercial banking president: ‘Come on back, the water’s fine’\n",
            "Allie wants to layer intelligence on top of factory floors\n",
            "Bird acquires Spin scooters from Tier for $19M\n",
            "Apple’s Lisa Jackson explains how going carbon neutral by 2030 is good business\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Base URL for TechCrunch articles\n",
        "BASE_URL = \"https://techcrunch.com/page/{}/\"\n",
        "\n",
        "# Initialize variables\n",
        "articles_collected = 0\n",
        "page_number = 1\n",
        "titles = []\n",
        "\n",
        "while articles_collected < 1000:\n",
        "    response = requests.get(BASE_URL.format(page_number))\n",
        "\n",
        "    # If response is not successful, exit\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to retrieve page {}. Exiting.\".format(page_number))\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all article titles on the page\n",
        "    article_elements = soup.find_all('h2', class_='post-block__title')\n",
        "\n",
        "    for article in article_elements:\n",
        "        title = article.get_text().strip()  # Extract and clean the title text\n",
        "        titles.append(title)\n",
        "        articles_collected += 1\n",
        "\n",
        "        # Break the loop if we have collected 1000 articles\n",
        "        if articles_collected >= 1000:\n",
        "            break\n",
        "\n",
        "    # Move to the next page\n",
        "    page_number += 1\n",
        "\n",
        "# Print the top 5 titles\n",
        "for i in range(5):\n",
        "    print(titles[i])\n",
        "print (len(titles))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M2fED8Yj0Ga",
        "outputId": "b9563d32-5bca-4f5a-9960-24243373a117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: Rhetorical relations for information retrieval\n",
            "Venue: August 2012, pp 931–940\n",
            "Year: 931–940\n",
            "Authors: \n",
            "---------\n",
            "Title: Differential Privacy for Information Retrieval\n",
            "Venue: October 2017, pp 325–326\n",
            "Year: 325–326\n",
            "Authors: \n",
            "---------\n",
            "Title: Modeling Term Associations for Probabilistic Information Retrieval\n",
            "Venue: Article No.: 7, pp 1–47\n",
            "Year: 1–47\n",
            "Authors: \n",
            "---------\n",
            "Title: Interactive Information Retrieval: An Evaluation Perspective\n",
            "Venue: March 2016, pp 151\n",
            "Year: 151\n",
            "Authors: \n",
            "---------\n",
            "Title: Statistical Significance Testing in Information Retrieval: Theory and Practice\n",
            "Venue: September 2013, pp 2\n",
            "Year: 2\n",
            "Authors: \n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://dl.acm.org/action/doSearch?AllField=information+retrieval&startPage={}\"\n",
        "\n",
        "def get_article_details(page_number):\n",
        "    articles = []\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    response = requests.get(BASE_URL.format(page_number), headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(response.status_code)\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    for paper in soup.find_all('div', class_='issue-item__content'):\n",
        "        title = paper.find('h5').get_text().strip() if paper.find('h5') else \"\"\n",
        "\n",
        "        venue = paper.find('span', class_='dot-separator').get_text().strip() if paper.find('span', class_='dot-separator') else \"\"\n",
        "        # Fix the year extraction\n",
        "        year = venue.split()[-1] if venue and any(char.isdigit() for char in venue) else \"\"\n",
        "\n",
        "        # Try a broader approach to catch authors if the previous approach doesn't work\n",
        "        authors = [author.get_text().strip() for author in paper.find_all('a', class_='author-name')] or [a.get_text().strip() for a in paper.find_all('a') if 'author' in a['href']]\n",
        "\n",
        "        abstract = \"\"  # ACM's initial search results do not display abstracts\n",
        "\n",
        "        articles.append({\n",
        "            \"Title\": title,\n",
        "            \"Venue\": venue,\n",
        "            \"Year\": year,\n",
        "            \"Authors\": \", \".join(authors),\n",
        "            \"Abstract\": abstract\n",
        "        })\n",
        "\n",
        "    #print(articles)\n",
        "    return articles\n",
        "\n",
        "\n",
        "collected_data = []\n",
        "page = 1\n",
        "\n",
        "while len(collected_data) < 10: #(we can change 10 to 1000 to get 1000 papers)\n",
        "    collected_data.extend(get_article_details(page))\n",
        "    page += 1\n",
        "    time.sleep(2)  # To be polite to the server\n",
        "\n",
        "# Print the first 5 entries for verification\n",
        "for i in range(5):\n",
        "    paper = collected_data[i]\n",
        "    print(\"Title:\", paper[\"Title\"])\n",
        "    print(\"Venue:\", paper[\"Venue\"])\n",
        "    print(\"Year:\", paper[\"Year\"])\n",
        "    print(\"Authors:\", paper[\"Authors\"])\n",
        "    print(\"---------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QC-Q59UWj0Gb",
        "outputId": "75bd856f-e9b8-4cd6-9284-9fd05f52460f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCQpbJnwTxAB"
      },
      "source": [
        "Do either of the question-4 tasks given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymVNKVi9-pb",
        "outputId": "dd29e4c9-00b1-4541-99b2-4577b01b45d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Username                Time  \\\n",
            "0      nasa 2023-09-19 16:46:27   \n",
            "1      nasa 2023-09-17 14:37:27   \n",
            "2      nasa 2023-09-16 18:34:49   \n",
            "3      nasa 2023-09-15 21:46:07   \n",
            "4      nasa 2023-09-14 14:01:14   \n",
            "..      ...                 ...   \n",
            "95     nasa 2023-06-18 12:36:23   \n",
            "96     nasa 2023-06-17 17:15:08   \n",
            "97     nasa 2023-06-16 19:54:15   \n",
            "98     nasa 2023-06-15 20:50:54   \n",
            "99     nasa 2023-06-14 18:50:49   \n",
            "\n",
            "                                              Caption  \n",
            "0   Moonlight Sonata⁣\\n ⁣\\nThis new mosaic reveals...  \n",
            "1   Sunny, thank you for the sunshine bouquet ☀️⁣\\...  \n",
            "2   F is for friends who do stuff together ⁣\\n⁣\\nJ...  \n",
            "3   The power of three will let us see\\n \\n@nasahu...  \n",
            "4   If we could take a baby picture of our Sun, it...  \n",
            "..                                                ...  \n",
            "95  Happy Father’s Day, Sun ☀️⁣\\n⁣\\nOur Sun may be...  \n",
            "96  Jupiter in a flash ⚡⁣\\n⁣\\nOur @NASASolarSystem...  \n",
            "97  Highly irregular 🧐\\n\\nThis @NASAHubble image f...  \n",
            "98  Beneath the most reflective surface in our sol...  \n",
            "99  Why is Venus so hot? ⁣\\n\\nThe planet’s thick C...  \n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import instaloader\n",
        "import pandas as pd\n",
        "\n",
        "def get_instagram_posts(profile_name, posts_count=1000, my_username=None, my_password=None):\n",
        "    \"\"\"\n",
        "    Get posts from an Instagram profile using instaloader.\n",
        "\n",
        "    Parameters:\n",
        "    - profile_name: Name of the Instagram profile to fetch posts from.\n",
        "    - posts_count: Total number of posts to fetch (default is 1000).\n",
        "    - my_username: Your Instagram username (default is None).\n",
        "    - my_password: Your Instagram password (default is None).\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with columns: 'Username', 'Time', 'Caption'.\n",
        "    \"\"\"\n",
        "\n",
        "    loader = instaloader.Instaloader()\n",
        "\n",
        "    # If username and password are provided, login to Instagram\n",
        "    if my_username and my_password:\n",
        "        loader.login(my_username, my_password)\n",
        "\n",
        "    # Fetch the desired Instagram profile\n",
        "    profile = instaloader.Profile.from_username(loader.context, profile_name)\n",
        "\n",
        "    data = []\n",
        "    for post in profile.get_posts():\n",
        "        if len(data) >= posts_count:\n",
        "            break\n",
        "\n",
        "        data.append({\n",
        "            'Username': post.owner_username,\n",
        "            'Time': post.date_utc,\n",
        "            'Caption': post.caption\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Test the function\n",
        "dataframe = get_instagram_posts('nasa', posts_count=100)\n",
        "print(dataframe)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}